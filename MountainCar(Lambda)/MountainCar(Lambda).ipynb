{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa(Î»)\n",
    "---\n",
    "Very few changes in the ideas already presented in this chapter are required in order to extend eligibility-traces to action-value methods.\n",
    "\n",
    "<img src=\"Sarsa(lambda).png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from TileCoding import *\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VELOCITY_BOUND = [-0.07, 0.07]\n",
    "POSITION_BOUND = [-1.2, 0.5]\n",
    "ACTIONS = [-1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "    \n",
    "    def __init__(self, stepSize, numOfTilings=8, maxSize=2048, lam=0.9, gamma=0.9):\n",
    "        self.maxSize = maxSize\n",
    "        self.numOfTilings = numOfTilings\n",
    "\n",
    "        # divide step size equally to each tiling\n",
    "        self.stepSize = stepSize / numOfTilings  # learning rate for each tile\n",
    "\n",
    "        self.hashTable = IHT(maxSize)\n",
    "\n",
    "        # weight for each tile\n",
    "        self.weights = np.zeros(maxSize)\n",
    "        \n",
    "        # trace vector\n",
    "        self.z = np.zeros(maxSize)\n",
    "        \n",
    "        self.lam = lam\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # position and velocity needs scaling to satisfy the tile software\n",
    "        self.positionScale = self.numOfTilings / (POSITION_BOUND[1] - POSITION_BOUND[0])\n",
    "        self.velocityScale = self.numOfTilings / (VELOCITY_BOUND[1] - VELOCITY_BOUND[0])\n",
    "\n",
    "    # get indices of active tiles for given state and action\n",
    "    def getActiveTiles(self, position, velocity, action):\n",
    "        # I think positionScale * (position - position_min) would be a good normalization.\n",
    "        # However positionScale * position_min is a constant, so it's ok to ignore it.\n",
    "        activeTiles = tiles(self.hashTable, self.numOfTilings,\n",
    "                            [self.positionScale * position, self.velocityScale * velocity],\n",
    "                            [action])\n",
    "        return activeTiles\n",
    "\n",
    "    # estimate the value of given state and action\n",
    "    def value(self, position, velocity, action):\n",
    "        if position == POSITION_BOUND[1]:\n",
    "            return 0.0\n",
    "        activeTiles = self.getActiveTiles(position, velocity, action)\n",
    "        return np.sum(self.weights[activeTiles])\n",
    "\n",
    "    # learn with given state, action and target\n",
    "    def update(self, position, velocity, action, target):\n",
    "\n",
    "        activeTiles = self.getActiveTiles(position, velocity, action)\n",
    "        \n",
    "        # update traces\n",
    "        self.z *= self.gamma*self.lam\n",
    "        self.z[activeTiles] += 1\n",
    "        # update weights\n",
    "        estimation = np.sum(self.weights[activeTiles])\n",
    "        delta = self.stepSize * (target - estimation)\n",
    "        self.weights += self.stepSize*delta*self.z\n",
    "        \n",
    "    # get the # of steps to reach the goal under current state value function\n",
    "    def costToGo(self, position, velocity):\n",
    "        costs = []\n",
    "        for action in ACTIONS:\n",
    "            costs.append(self.value(position, velocity, action))\n",
    "        return -np.max(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar:\n",
    "    \n",
    "    def __init__(self, n=1, exp_rate=0.1, gamma=1, debug=True):\n",
    "        self.actions = [-1, 0, 1]  # reverse, 0 and forward throttle\n",
    "        self.state = (-0.5, 0)  # position, velocity\n",
    "        self.exp_rate = exp_rate\n",
    "        self.gamma = gamma\n",
    "        self.end = False\n",
    "        self.n = n  # step of learning\n",
    "        self.debug = debug\n",
    "        \n",
    "    def reset(self):\n",
    "        pos = np.random.uniform(-0.6, -0.4)\n",
    "        self.end = False\n",
    "        self.state = (pos, 0)\n",
    "        \n",
    "    def takeAction(self, action):\n",
    "        pos, vel = self.state\n",
    "        \n",
    "        vel_new = vel + 0.001*action - 0.0025*np.cos(3*pos)\n",
    "        vel_new = min(max(vel_new, VELOCITY_BOUND[0]), VELOCITY_BOUND[1])\n",
    "        \n",
    "        pos_new = pos + vel_new\n",
    "        pos_new = min(max(pos_new, POSITION_BOUND[0]), POSITION_BOUND[1])\n",
    "        \n",
    "        if pos_new == POSITION_BOUND[0]:\n",
    "            # reach leftmost, set speed to 0\n",
    "            vel_new = 0\n",
    "        nxtState = (pos_new, vel_new)\n",
    "        return nxtState\n",
    "    \n",
    "    def chooseAction(self, valueFunc, state):\n",
    "        # choose an action based on given state\n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            # random action\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            values = {}\n",
    "            for a in self.actions:\n",
    "                pos, vel = state\n",
    "                value = valueFunc.value(pos, vel, a)\n",
    "                values[a] = value\n",
    "            return np.random.choice([k for k, v in values.items() if v==max(values.values())])\n",
    "        \n",
    "    def giveReward(self, state):\n",
    "        # give reward based on state\n",
    "        pos, _ = state\n",
    "        if pos == POSITION_BOUND[1]:\n",
    "            self.end = True\n",
    "            return 0\n",
    "        return -1\n",
    "        \n",
    "    def play(self, valueFunction, rounds=1):\n",
    "        for rnd in range(1, rounds+1):\n",
    "            self.reset()\n",
    "            while True:\n",
    "                currentState = self.state\n",
    "                action = self.chooseAction(valueFunction, currentState)\n",
    "                nxtState = self.takeAction(action)  # next state\n",
    "                reward = self.giveReward(nxtState)  # next state-reward\n",
    "                nxtAction = self.chooseAction(valueFunction, nxtState)\n",
    "                \n",
    "                target = reward + valueFunction.value(nxtState[0], nxtState[1], nxtAction)\n",
    "                valueFunction.update(currentState[0], currentState[1], action, target)\n",
    "                \n",
    "                self.state = nxtState\n",
    "                \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
