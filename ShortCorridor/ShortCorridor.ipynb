{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Corridor with Switched Actions\n",
    "---\n",
    "Consider the small corridor grid world shown inset in the graph below. The reward is -1 per step, as usual. In each of the three nonterminal states there are only two actions, right and left. These actions have their usual consequences in the first and third states (left causes no movement in the first state), but in the second state they are reversed, so that right moves to the left and left moves to the right. The problem is difficult because all the states appear identical under the function approximation. In particular, we define `x(s, right) = [1, 0]` and `x(s, left) = [0, 1]`, for all s.\n",
    "\n",
    "$$J(\\theta) = V_{\\pi_\\theta}(S)$$\n",
    "\n",
    "<img src=\"corridor.png\" width=\"600\">\n",
    "\n",
    "MC Policy Gradient\n",
    "---\n",
    "<img src=\"mc_policy_gradient.png\" width=\"600\">\n",
    "<img src=\"h.png\" width=\"300\">\n",
    "<img src=\"policy.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class ShortCorridor:\n",
    "    def __init__(self, alpha=0.2, gamma=0.8):\n",
    "        self.actions = [\"left\", \"right\"]\n",
    "        self.x = np.array([[0, 1], [1, 0]])  # left|s, right|s\n",
    "        self.theta = np.array([-1.47, 1.47])\n",
    "        self.state = 0  # initial state 0\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def softmax(self, vector):\n",
    "        return np.exp(vector)/sum(np.exp(vector))\n",
    "        \n",
    "    def chooseAction(self):\n",
    "        h = np.dot(self.theta, self.x)\n",
    "        prob = self.softmax(h)  # left, right probability for all state\n",
    "        \n",
    "        action = np.random.choice(self.actions, p=prob)\n",
    "        return action\n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        if self.state == 0:\n",
    "            nxtState = 0 if action == \"left\" else 1\n",
    "        elif self.state == 1:\n",
    "            nxtState = 2 if action == \"left\" else 0  # reversed\n",
    "        elif self.state == 2:\n",
    "            nxtState = 1 if action == \"left\" else 3\n",
    "        else:\n",
    "            nxtState = 2 if action == \"left\" else 3\n",
    "        return nxtState\n",
    "    \n",
    "    def giveReward(self):\n",
    "        if self.state == 3:\n",
    "            return 0\n",
    "        return -1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "    \n",
    "    def run(self, rounds=100):\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        for i in range(1, rounds+1):\n",
    "            reward_sum = 0\n",
    "            while True:\n",
    "                action = self.chooseAction()\n",
    "                nxtState = self.takeAction(action)\n",
    "                reward = self.giveReward()\n",
    "                reward_sum += reward\n",
    "\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                self.state = nxtState\n",
    "                # game end\n",
    "                if self.state == 3:\n",
    "                    T = len(rewards)\n",
    "                    for t in range(T):\n",
    "                        # calculate G\n",
    "                        G = 0\n",
    "                        for k in range(t+1, T):\n",
    "                            G += np.power(self.gamma, k-t-1)*rewards[k]\n",
    "                \n",
    "                        j = 1 if actions[t] == \"right\" else 0  # dev on particular state\n",
    "                        h = np.dot(self.theta, self.x)\n",
    "                        prob = self.softmax(h)\n",
    "                        grad = self.x[:, j] - np.dot(self.x, prob)\n",
    "\n",
    "                        self.theta += self.alpha*np.power(self.gamma, t)*G*grad\n",
    "                    # reset \n",
    "                    self.state = 0\n",
    "                    actions = []\n",
    "                    rewards = []\n",
    "                    \n",
    "                    if i % 50 == 0: \n",
    "                        print(\"round {}: current prob {} reward {}\".format(i, prob, reward_sum))\n",
    "                        reward_sum = 0\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ShortCorridor(alpha=2e-4, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 50: current prob [0.85449004 0.14550996] reward -35\n",
      "round 100: current prob [0.84093479 0.15906521] reward -47\n",
      "round 150: current prob [0.82282785 0.17717215] reward -5\n",
      "round 200: current prob [0.76987516 0.23012484] reward -18\n",
      "round 250: current prob [0.7454774 0.2545226] reward -5\n",
      "round 300: current prob [0.67866504 0.32133496] reward -14\n",
      "round 350: current prob [0.64634627 0.35365373] reward -20\n",
      "round 400: current prob [0.58537224 0.41462776] reward -55\n",
      "round 450: current prob [0.56361743 0.43638257] reward -7\n",
      "round 500: current prob [0.56006606 0.43993394] reward -20\n",
      "round 550: current prob [0.53366404 0.46633596] reward -3\n",
      "round 600: current prob [0.50642727 0.49357273] reward -13\n",
      "round 650: current prob [0.49755331 0.50244669] reward -13\n",
      "round 700: current prob [0.49075649 0.50924351] reward -9\n",
      "round 750: current prob [0.46767641 0.53232359] reward -9\n",
      "round 800: current prob [0.46414269 0.53585731] reward -7\n",
      "round 850: current prob [0.4328104 0.5671896] reward -14\n",
      "round 900: current prob [0.45682663 0.54317337] reward -12\n",
      "round 950: current prob [0.4503979 0.5496021] reward -3\n",
      "round 1000: current prob [0.42265094 0.57734906] reward -5\n"
     ]
    }
   ],
   "source": [
    "sc.run(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15595021, -0.15595021])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42265094, 0.57734906])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = np.dot(sc.theta, sc.x)\n",
    "sc.softmax(h)  # left, right probability for all state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
