{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Corridor with Switched Actions\n",
    "---\n",
    "Consider the small corridor gridworld shown inset in the graph below. The reward is -1 per step, as usual. In each of the three nonterminal states there are only two actions, right and left. These actions have their usual consequences in the first and third states (left causes no movement in the first state), but in the second state they are reversed, so that right moves to the left and left moves to the right. The problem is diffcult because all the states appear identical under the function approximation. In particular, we define `x(s, right) = [1, 0]` and `x(s, left) = [0, 1]`, for all s.\n",
    "\n",
    "<img src=\"corridor.png\" width=\"600\">\n",
    "\n",
    "MC Policy Gradient\n",
    "---\n",
    "<img src=\"mc_policy_gradient.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortCorridor:\n",
    "    def __init__(self, alpha=0.2, gamma=0.8):\n",
    "        self.actions = [\"left\", \"right\"]\n",
    "        self.x = np.array([[0, 1], [1, 0]])  # left|s, right|s\n",
    "        self.theta = np.array([-1.47, 1.47])\n",
    "        self.state = 0  # initial state 0\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def softmax(self, vector):\n",
    "        return np.exp(vector)/sum(np.exp(vector))\n",
    "        \n",
    "    def chooseAction(self):\n",
    "        h = np.dot(self.theta, self.x)\n",
    "        prob = self.softmax(h)  # left, right probability for all state\n",
    "        \n",
    "        action = np.random.choice(self.actions, p=prob)\n",
    "        return action\n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        if self.state == 0:\n",
    "            nxtState = 0 if action == \"left\" else 1\n",
    "        elif self.state == 1:\n",
    "            nxtState = 2 if action == \"left\" else 0  # reversed\n",
    "        elif self.state == 2:\n",
    "            nxtState = 1 if action == \"left\" else 3\n",
    "        else:\n",
    "            nxtState = 2 if action == \"left\" else 3\n",
    "        return nxtState\n",
    "    \n",
    "    def giveReward(self):\n",
    "        if self.state == 3:\n",
    "            return 0\n",
    "        return -1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "    \n",
    "    def run(self, rounds=100):\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        for i in range(rounds):\n",
    "            while True:\n",
    "                action = self.chooseAction()\n",
    "                nxtState = self.takeAction(action)\n",
    "                reward = self.giveReward()\n",
    "\n",
    "#                 print(\"state {} action {} reward {} next_state {}\".format(self.state, action, reward, nxtState))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                self.state = nxtState\n",
    "                # game end\n",
    "                if self.state == 3:\n",
    "                    print(\"end state\")\n",
    "                    T = len(rewards)\n",
    "                    for t in range(T):\n",
    "                        # calculate G\n",
    "                        G = 0\n",
    "                        for k in range(t+1, T):\n",
    "                            G += np.power(self.gamma, k-t-1)*rewards[k]\n",
    "                \n",
    "                        j = 1 if actions[t] == \"right\" else 0\n",
    "                        h = np.dot(self.theta, self.x)\n",
    "                        prob = self.softmax(h)\n",
    "                        grad = self.x[:, j] - np.dot(self.x, prob)\n",
    "\n",
    "                        self.theta += self.alpha*np.power(self.gamma, t)*G*grad\n",
    "                    # reset \n",
    "                    self.state = 0\n",
    "                    actions = []\n",
    "                    rewards = []\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = ShortCorridor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n",
      "end state\n"
     ]
    }
   ],
   "source": [
    "sc.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
