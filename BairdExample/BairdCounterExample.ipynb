{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baird Counter Example\n",
    "---\n",
    "To convert them to semi-gradient form, we simply replace the update to an array `(V or Q)` to an update to a weight vector `(w)`, using the approximate value function (`vˆ` or `qˆ`) and its gradient. Many of these algorithms use the per-step `importance sampling ratio`:\n",
    "\n",
    "<img src=\"rho.png\" width=\"200\">\n",
    "\n",
    "<img src=\"weights_update.png\" width=\"300\">\n",
    "\n",
    "---\n",
    "The dashed action takes the system to one of the six upper states with equal probability, whereas the solid action takes the system to the seventh state. The `behavior policy b` selects the dashed and solid actions with probabilities `6/7` and `1/7`, so that the next-state distribution under it is uniform (the same for all nonterminal states), which is also the starting distribution for each episode. The target policy `π` always takes the solid action, and so the on-policy distribution (for `π`) is concentrated in the seventh state. The reward is `zero` on all transitions. The discount rate is `γ = 0.99`.\n",
    "\n",
    "<img src=\"Baird.png\" width=\"600\">\n",
    "\n",
    "- [Target Policy & Behaviour Policy](https://stats.stackexchange.com/questions/410131/in-rl-why-using-a-behavior-policy-instead-of-the-target-policy-for-an-episode-i)\n",
    "- [Target Policy & Behaviour Policy2](https://stats.stackexchange.com/questions/237085/how-to-correctly-compute-rho-in-reinforcement-learning-with-importance-sampli?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATES = range(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baird:\n",
    "    \n",
    "    def __init__(self, gamma=0.99, alpha=0.01):\n",
    "        self.state = np.random.choice(STATES)\n",
    "        self.prob = 1.0/7\n",
    "        self.actions = [\"solid\", \"dash\"]\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.features = np.zeros((len(STATES), 8))  # n_states x n_weights (this is the representation of states)\n",
    "        for i in range(len(STATES)):\n",
    "            if i == 6:\n",
    "                self.features[i, -2] = 1\n",
    "                self.features[i, -1] = 2\n",
    "            else:\n",
    "                self.features[i, i] = 2\n",
    "                self.features[i, -1] = 1\n",
    "        \n",
    "        self.weights = np.ones(8)\n",
    "        self.weights[-2] = 10\n",
    "        \n",
    "    def chooseAction(self):\n",
    "        if np.random.binomial(1, self.prob) == 1:\n",
    "            action = \"solid\"\n",
    "        else:\n",
    "            action = \"dash\"\n",
    "        return action\n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        if action == \"solid\":\n",
    "            nxtState = 6\n",
    "        else:\n",
    "            nxtState = np.random.choice(STATES[:-1])\n",
    "        return nxtState\n",
    "    \n",
    "    def value(self, state):\n",
    "        v = np.dot(ba.features[state, :], ba.weights)\n",
    "        return v\n",
    "    \n",
    "    def update(self, state, delta):\n",
    "        self.weights += delta*self.features[state, :]\n",
    "    \n",
    "    def run(self, rounds=100):\n",
    "        reward = 0\n",
    "        for _ in range(rounds):\n",
    "            action = self.chooseAction()\n",
    "            nxtState = self.takeAction(action)\n",
    "            \n",
    "            if action == \"dash\":\n",
    "                rho = 0\n",
    "            else:\n",
    "                rho = 1/self.prob\n",
    "#             rho = 1\n",
    "            \n",
    "            delta = reward + self.gamma*self.value(nxtState) - self.value(self.state)\n",
    "            delta *= self.alpha*rho\n",
    "            self.update(self.state, delta)\n",
    "                        \n",
    "            self.state = nxtState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 2. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 2. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 2. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 2. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 2. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 2.]]\n",
      "\n",
      "[ 1.  1.  1.  1.  1.  1. 10.  1.]\n"
     ]
    }
   ],
   "source": [
    "ba = Baird()\n",
    "print(ba.features)\n",
    "print()\n",
    "print(ba.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ba = Baird()\n",
    "ba.run(rounds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 98.66944559,  87.82605498,  83.72652029,  96.42341865,\n",
       "       113.56688306,  83.48255486,   5.62034809, 271.08813489])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
