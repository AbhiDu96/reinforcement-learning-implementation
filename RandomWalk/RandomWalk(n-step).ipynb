{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randow Walk\n",
    "---\n",
    "## n-step TD Method\n",
    "\n",
    "<img style=\"float\" src=\"rw-game.png\" alt=\"drawing\" width=\"700\"/>\n",
    "\n",
    "In this MRP, all episodes start in the center state, C, then proceed either left or right by one state on each step, with equal probability. Episodes terminate either on the extreme left or the extreme right. When an episode terminates on the right, a reward of +1 occurs; all other rewards are zero.\n",
    "\n",
    "<img style=\"float\" src=\"n-step.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 states \n",
    "NUM_STATES = 19\n",
    "START = 9\n",
    "END_0 = 0\n",
    "END_1 = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalk:\n",
    "    \n",
    "    def __init__(self, n, start=START, end=False, exp_rate=0.3, lr=0.1, gamma=1):\n",
    "        self.actions = [\"left\", \"right\"]\n",
    "        self.state = start  # current state\n",
    "        self.end = end\n",
    "        self.n = n\n",
    "        self.exp_rate = exp_rate\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.state_actions = []\n",
    "        # init q estimates\n",
    "        self.Q_values = {}\n",
    "        for i in range(NUM_STATES):\n",
    "            self.Q_values[i] = {}\n",
    "            for a in self.actions:\n",
    "                self.Q_values[i][a] = 0\n",
    "                \n",
    "    def chooseAction(self):\n",
    "        # epsilon-greedy\n",
    "        mx_nxt_reward = -999\n",
    "        action = \"\"\n",
    "        \n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                current_position = self.state\n",
    "                nxt_reward = self.Q_values[current_position][a]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "        return action \n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        new_state = self.state\n",
    "        if not self.end:\n",
    "            if action == \"left\":\n",
    "                new_state = self.state-1\n",
    "            else:\n",
    "                new_state = self.state+1\n",
    "            \n",
    "            if new_state in [END_0, END_1]:\n",
    "                self.end = True\n",
    "        self.state = new_state\n",
    "        return self.state\n",
    "    \n",
    "    def giveReward(self):\n",
    "        if self.state == END_0:\n",
    "            return 0\n",
    "        if self.state == END_1:\n",
    "            return 1\n",
    "        # other states\n",
    "        return 0\n",
    "        \n",
    "    def play(self, rounds=100):\n",
    "        for _ in range(rounds):\n",
    "            t = 0\n",
    "            T = np.inf\n",
    "            states = [self.state]\n",
    "            rewards = [0]\n",
    "            while True:\n",
    "                if t < T:\n",
    "                    action = self.chooseAction()\n",
    "                    self.state_actions.append((self.state, action))\n",
    "                    \n",
    "                    state = self.takeAction(action)\n",
    "                    reward = self.giveReward()\n",
    "                    \n",
    "                    states.append(state)\n",
    "                    rewards.append(reward)\n",
    "                    \n",
    "                    if self.end:\n",
    "                        T = t+1\n",
    "                tau = t - self.n + 1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    for i in range(tau+1, min(tau+self.n+1, T+1)):\n",
    "                        G += np.power(self.gamma, i-tau-1)*rewards[i]\n",
    "                    if tau+self.n < T:\n",
    "                        state_action = self.state_actions[tau+self.n]\n",
    "                        G += np.power(self.gamma, self.n)*self.Q_values[state_action[0]][state_action[1]]\n",
    "                    # update Q values\n",
    "                    state_action = self.state_actions[tau]\n",
    "                    self.Q_values[state_action[0]][state_action[1]] += self.lr*(G-self.Q_values[state_action[0]][state_action[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
