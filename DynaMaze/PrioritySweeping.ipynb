{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority Sweeping\n",
    "---\n",
    "<img style=\"float:left\" src=\"ps.png\" alt=\"drawing\" width=\"600\"/>\n",
    "> Only an update along a transition into the state just prior to the goal, or from it, will change any values. It is natural to prioritize the updates according to a measure of their `urgency`, and perform them in order of priority. This is the idea behind `prioritized sweeping`. A queue is maintained of every stateâ€“action pair whose estimated value would change nontrivially if updated, prioritized by the size of the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 6\n",
    "COLS = 9\n",
    "S = (2, 0)\n",
    "G = (0, 8)\n",
    "BLOCKS = [(1, 2), (2, 2), (3, 2), (0, 7), (1, 7), (2, 7), (4, 5)]\n",
    "ACTIONS = [\"left\", \"up\", \"right\", \"down\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rows = ROWS\n",
    "        self.cols = COLS\n",
    "        self.start = S\n",
    "        self.goal = G\n",
    "        self.blocks = BLOCKS\n",
    "        self.state = S\n",
    "        self.end = False\n",
    "        # init maze\n",
    "        self.maze = np.zeros((self.rows, self.cols))\n",
    "        for b in self.blocks:\n",
    "            self.maze[b] = -1\n",
    "            \n",
    "    def nxtPosition(self, action):\n",
    "        r, c = self.state\n",
    "        if action == \"left\":\n",
    "            c -= 1\n",
    "        elif action == \"right\":\n",
    "            c += 1\n",
    "        elif action == \"up\":\n",
    "            r -= 1\n",
    "        else:\n",
    "            r += 1\n",
    "        \n",
    "        if (r >= 0 and r <= self.rows-1) and (c >= 0 and c <= self.cols-1):\n",
    "            if (r, c) not in self.blocks:\n",
    "                self.state = (r, c)\n",
    "        return self.state\n",
    "    \n",
    "    def giveReward(self):\n",
    "        if self.state == self.goal:\n",
    "            self.end = True\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def showMaze(self):\n",
    "        self.maze[self.state] = 1\n",
    "        for i in range(0, self.rows):\n",
    "            print('-------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, self.cols):\n",
    "                if self.maze[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.maze[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.maze[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityAgent:\n",
    "    \n",
    "    def __init__(self, exp_rate=0.3, lr=0.1, n_steps=5, episodes=1, theta=0):\n",
    "        self.maze = Maze()\n",
    "        self.state = S\n",
    "        self.actions = ACTIONS\n",
    "        self.state_actions = []  # state & action track\n",
    "        self.exp_rate = exp_rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.steps = n_steps\n",
    "        self.episodes = episodes  # number of episodes going to play\n",
    "        self.steps_per_episode = []\n",
    "        \n",
    "        self.Q_values = {}\n",
    "        # model function\n",
    "        self.model = {}\n",
    "        for row in range(ROWS):\n",
    "            for col in range(COLS):\n",
    "                self.Q_values[(row, col)] = {}\n",
    "                self.model[(row, col)] = {}\n",
    "                for a in self.actions:\n",
    "                    self.Q_values[(row, col)][a] = 0\n",
    "                    self.model[(row, col)][a] = (0, (0, 0))  # reward & next state\n",
    "        \n",
    "        # for priority sweeping\n",
    "        self.theta = theta\n",
    "        self.queue = PriorityQueue()\n",
    "        self.predecessors = {}  # nxtState -> list[(curState, Action)...]\n",
    "        \n",
    "    def chooseAction(self):\n",
    "        # epsilon-greedy\n",
    "        mx_nxt_reward = -999\n",
    "        action = \"\"\n",
    "        \n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            current_position = self.state\n",
    "            # if all actions have same value, then select randomly\n",
    "            if len(set(self.Q_values[current_position].values())) == 1:\n",
    "                action = np.random.choice(self.actions)\n",
    "            else:\n",
    "                for a in self.actions:\n",
    "                    nxt_reward = self.Q_values[current_position][a]\n",
    "                    if nxt_reward >= mx_nxt_reward:\n",
    "                        action = a\n",
    "                        mx_nxt_reward = nxt_reward\n",
    "        return action\n",
    "    \n",
    "    def reset(self):\n",
    "        self.maze = Maze()\n",
    "        self.state = S\n",
    "        self.state_actions = []\n",
    "    \n",
    "    def play(self):\n",
    "        for ep in range(self.episodes):    \n",
    "            while not self.maze.end:\n",
    "\n",
    "                action = self.chooseAction()\n",
    "                self.state_actions.append((self.state, action))\n",
    "\n",
    "                nxtState = self.maze.nxtPosition(action)\n",
    "                reward = self.maze.giveReward()\n",
    "                \n",
    "                # update priority queue\n",
    "                tmp_diff = reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[self.state][action]\n",
    "                if tmp_diff > self.theta:\n",
    "                    self.queue.put((-tmp_diff, (self.state, action)))  # -diff -> (state, action) pop the smallest\n",
    "                self.Q_values[self.state][action] += self.lr*tmp_diff\n",
    "\n",
    "                # update model & predecessors\n",
    "                self.model[self.state][action] = (reward, nxtState)\n",
    "                if nxtState not in self.predecessors.keys():\n",
    "                    self.predecessors[nxtState] = [(self.state, action)]\n",
    "                else:\n",
    "                    self.predecessors[nxtState] = self.predecessors[nxtState].append((self.state, action))\n",
    "                self.state = nxtState\n",
    "\n",
    "                # loop n times to randomly update Q-value\n",
    "                for _ in range(self.steps):\n",
    "                    if self.queue.empty():\n",
    "                        break\n",
    "                    _state, _action = self.queue.get()[1]\n",
    "                    _reward, _nxtState = self.model[_state][_action]\n",
    "                    self.Q_values[_state][_action] += self.lr*(_reward + np.max(list(self.Q_values[_nxtState].values())) - self.Q_values[_state][_action])\n",
    "                    \n",
    "                    # loop for all state, action predicted lead to _state\n",
    "                    if _state not in self.predecessors.keys():\n",
    "                        continue                      \n",
    "                    pre_state_action_list = self.predecessors[_state]\n",
    "                    for (pre_state, pre_action) in pre_state_action_list:\n",
    "                        pre_reward, _ = self.model[pre_state][pre_action]\n",
    "                        pre_tmp_diff = pre_reward + np.max(list(self.Q_values[_state].values())) - self.Q_values[pre_state][pre_action]\n",
    "                        if pre_tmp_diff > self.theta:\n",
    "                            self.queue.put((-pre_tmp_diff, (pre_state, pre_action)))\n",
    "            # end of game\n",
    "            if ep % 10 == 0:\n",
    "                print(\"episode\", ep)\n",
    "            self.steps_per_episode.append(len(self.state_actions))\n",
    "            self.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = PriorityQueue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.put((20, ((1, 1), \"l\")))\n",
    "q.put((3, ((2, 1), \"l\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a = q.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = q.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, ((1, 1), 'l'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
