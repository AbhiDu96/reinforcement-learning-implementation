{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offline Lambda\n",
    "---\n",
    "<img src=\"illustration.png\" width=\"400\">\n",
    "---\n",
    "The backup digram for `TD(λ)`. If `λ = 0`, then the overall update reduces to its first component, the one-step TD update, whereas if `λ = 1`, then the overall update reduces to its last component, the Monte Carlo update.\n",
    "Update Rule\n",
    "---\n",
    "<img src=\"Gtn.png\" width=\"600\">\n",
    "<img src=\"Gt.png\" width=\"400\">\n",
    "<img src=\"offline_lambda.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 19 states (not including the ending state)\n",
    "NUM_STATES = 19\n",
    "START = 9\n",
    "END_0 = 0\n",
    "END_1 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.weights = np.zeros(NUM_STATES+2)\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def value(self, state):\n",
    "        v = self.weights[state]\n",
    "        return v\n",
    "    \n",
    "    def learn(self, state, delta):\n",
    "        self.weights[state] += self.alpha*delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalk:\n",
    "    \n",
    "    def __init__(self, start=START, end=False, lmbda=0.4, debug=False):\n",
    "        self.actions = [\"left\", \"right\"]\n",
    "        self.state = start  # current state\n",
    "        self.end = end\n",
    "        self.lmbda = lmbda\n",
    "        self.states = []\n",
    "        self.reward = 0\n",
    "        self.debug = debug\n",
    "        self.rate_truncate = 1e-3\n",
    "                \n",
    "    def chooseAction(self):    \n",
    "        action = np.random.choice(self.actions)\n",
    "        return action \n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        new_state = self.state\n",
    "        if not self.end:\n",
    "            if action == \"left\":\n",
    "                new_state = self.state-1\n",
    "            else:\n",
    "                new_state = self.state+1\n",
    "            \n",
    "            if new_state in [END_0, END_1]:\n",
    "                self.end = True\n",
    "        self.state = new_state\n",
    "        return self.state\n",
    "    \n",
    "    def giveReward(self):\n",
    "        if self.state == END_0:\n",
    "            return -1\n",
    "        if self.state == END_1:\n",
    "            return 1\n",
    "        # other states\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = START\n",
    "        self.end = False\n",
    "        self.states = []\n",
    "    \n",
    "    def gt2tn(self, valueFunc, start, end):\n",
    "        # only the last reward is non-zero\n",
    "        reward = self.reward if end == len(self.states)-1 else 0\n",
    "        state = self.states[end] \n",
    "        res = reward + valueFunc.value(state)\n",
    "        return res\n",
    "           \n",
    "    def play(self, valueFunc, rounds=100):\n",
    "        for _ in range(rounds):\n",
    "            self.reset()      \n",
    "            action = self.chooseAction()\n",
    "\n",
    "            self.states = [self.state]\n",
    "            while not self.end:\n",
    "                state = self.takeAction(action)  # next state\n",
    "                self.reward = self.giveReward()  # next state-reward\n",
    "\n",
    "                self.states.append(state)\n",
    "\n",
    "                action = self.chooseAction()\n",
    "            if self.debug:\n",
    "                print(\"total states {} end at {} reward {}\".format(len(self.states), self.state, self.reward))\n",
    "\n",
    "            # end of game, do forward update\n",
    "            T = len(self.states)-1\n",
    "            for t in range(T):\n",
    "                # start from time t\n",
    "                state = self.states[t]\n",
    "                gtlambda = 0\n",
    "                for n in range(1, T-t):\n",
    "                    # compute G_t:t+n\n",
    "                    gttn = self.gt2tn(valueFunc, t, t+n)\n",
    "                    gtlambda += np.power(self.lmbda, n-1)*gttn\n",
    "                \n",
    "                if np.power(self.lmbda, T-t-1) >= self.rate_truncate:\n",
    "                    gtlambda += np.power(self.lmbda, T-t-1)*self.reward\n",
    "                gtlambda *= 1-self.lmbda\n",
    "                delta = gtlambda - valueFunc.value(state)\n",
    "                valueFunc.learn(state, delta)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueFunc = ValueFunction(alpha=0.4)\n",
    "rw = RandomWalk(debug=False, lmbda=0.8)\n",
    "\n",
    "rw.play(valueFunc, rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.19378097, -0.18742823, -0.18252128, -0.17353533,\n",
       "       -0.15371629, -0.07428831, -0.02239971, -0.01035814, -0.00771267,\n",
       "        0.03685657,  0.04918544,  0.07999859,  0.1002818 ,  0.10664106,\n",
       "        0.11290332,  0.15059708,  0.1738342 ,  0.1828638 ,  0.19080638,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valueFunc.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1. , -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,\n",
       "        0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_state_values = np.arange(-20, 22, 2) / 20.0\n",
    "actual_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
